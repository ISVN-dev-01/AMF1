import pytest
import pandas as pd
import numpy as np
from pathlib import Path
import sys
import os

# Add src to path for imports
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src', 'features'))

class TestLabelGeneration:
    """Test suite for label generation"""
    
    @classmethod
    def setup_class(cls):
        """Setup test data"""
        cls.labels_file = Path('data/processed/labels.parquet')
        
        if cls.labels_file.exists():
            cls.labels_df = pd.read_parquet(cls.labels_file)
        else:
            # Create mock data for testing if file doesn't exist
            cls.labels_df = pd.DataFrame({
                'race_id': ['2024_1', '2024_1', '2024_2', '2024_2'],
                'driver_id': ['hamilton', 'verstappen', 'leclerc', 'sainz'],
                'is_pole': [1, 0, 0, 1],
                'is_race_winner': [0, 1, 1, 0],
                'quali_best_time': [78.123, 78.456, 77.890, 78.200],
                'race_position': [2, 1, 1, 3]
            })
    
    def test_required_columns_exist(self):
        """Test: Required label columns exist"""
        if self.labels_df.empty:
            pytest.skip("No labels data to test")
        
        required_columns = ['race_id', 'driver_id', 'is_pole', 'is_race_winner']
        missing_columns = [col for col in required_columns if col not in self.labels_df.columns]
        
        assert len(missing_columns) == 0, f"Missing required columns: {missing_columns}"
    
    def test_no_null_critical_fields(self):
        """Test: No null values in critical fields"""
        if self.labels_df.empty:
            pytest.skip("No labels data to test")
        
        # Check race_id
        null_race_ids = self.labels_df['race_id'].isnull().sum()
        assert null_race_ids == 0, f"Found {null_race_ids} null race_id values"
        
        # Check driver_id
        null_driver_ids = self.labels_df['driver_id'].isnull().sum()
        assert null_driver_ids == 0, f"Found {null_driver_ids} null driver_id values"
    
    def test_binary_flags_valid(self):
        """Test: Binary flags contain only 0/1 values"""
        if self.labels_df.empty:
            pytest.skip("No labels data to test")
        
        binary_columns = ['is_pole', 'is_race_winner']
        
        for col in binary_columns:
            if col in self.labels_df.columns:
                unique_values = set(self.labels_df[col].dropna().unique())
                valid_values = {0, 1, 0.0, 1.0}  # Allow both int and float
                
                invalid_values = unique_values - valid_values
                assert len(invalid_values) == 0, f"Invalid values in {col}: {invalid_values}"
    
    def test_one_pole_per_race(self):
        """Test: Each race has exactly one pole position"""
        if self.labels_df.empty or 'is_pole' not in self.labels_df.columns:
            pytest.skip("No pole position data to test")
        
        poles_per_race = self.labels_df.groupby('race_id')['is_pole'].sum()
        
        # Races with no pole positions
        races_no_pole = (poles_per_race == 0).sum()
        # Races with multiple pole positions  
        races_multi_pole = (poles_per_race > 1).sum()
        
        # Allow some flexibility - warn but don't fail for data quality issues
        if races_no_pole > 0:
            print(f"Warning: {races_no_pole} races with no pole position identified")
        
        if races_multi_pole > 0:
            print(f"Warning: {races_multi_pole} races with multiple pole positions")
            
        # Strict test: majority of races should have exactly 1 pole
        races_one_pole = (poles_per_race == 1).sum()
        total_races = len(poles_per_race)
        
        if total_races > 0:
            one_pole_percentage = races_one_pole / total_races
            assert one_pole_percentage > 0.8, f"Only {one_pole_percentage:.1%} of races have exactly one pole position"
    
    def test_one_winner_per_race(self):
        """Test: Each race has exactly one winner"""
        if self.labels_df.empty or 'is_race_winner' not in self.labels_df.columns:
            pytest.skip("No race winner data to test")
        
        winners_per_race = self.labels_df.groupby('race_id')['is_race_winner'].sum()
        
        # Races with no winners
        races_no_winner = (winners_per_race == 0).sum()
        # Races with multiple winners
        races_multi_winner = (winners_per_race > 1).sum()
        
        # Allow some flexibility - warn but don't fail for data quality issues
        if races_no_winner > 0:
            print(f"Warning: {races_no_winner} races with no winner identified")
        
        if races_multi_winner > 0:
            print(f"Warning: {races_multi_winner} races with multiple winners")
            
        # Strict test: majority of races should have exactly 1 winner
        races_one_winner = (winners_per_race == 1).sum()
        total_races = len(winners_per_race)
        
        if total_races > 0:
            one_winner_percentage = races_one_winner / total_races
            assert one_winner_percentage > 0.8, f"Only {one_winner_percentage:.1%} of races have exactly one winner"
    
    def test_qualifying_times_reasonable(self):
        """Test: Qualifying times are in reasonable range"""
        if self.labels_df.empty or 'quali_best_time' not in self.labels_df.columns:
            pytest.skip("No qualifying time data to test")
        
        quali_times = self.labels_df['quali_best_time'].dropna()
        
        if len(quali_times) == 0:
            pytest.skip("No valid qualifying times to test")
        
        # F1 qualifying times typically 60-120 seconds depending on track
        min_reasonable_time = 60.0  # 1 minute (very fast track)
        max_reasonable_time = 180.0  # 3 minutes (very slow track or wet conditions)
        
        times_too_fast = (quali_times < min_reasonable_time).sum()
        times_too_slow = (quali_times > max_reasonable_time).sum()
        
        total_times = len(quali_times)
        
        # Allow some outliers for special circumstances
        outlier_percentage = (times_too_fast + times_too_slow) / total_times
        
        assert outlier_percentage < 0.1, f"Too many outlier qualifying times: {outlier_percentage:.1%} outside {min_reasonable_time}-{max_reasonable_time}s range"
    
    def test_race_positions_valid(self):
        """Test: Race positions are valid integers"""
        if self.labels_df.empty or 'race_position' not in self.labels_df.columns:
            pytest.skip("No race position data to test")
        
        positions = self.labels_df['race_position'].dropna()
        
        if len(positions) == 0:
            pytest.skip("No valid race positions to test")
        
        # Check positions are positive integers
        non_positive = (positions <= 0).sum()
        assert non_positive == 0, f"Found {non_positive} non-positive race positions"
        
        # Check positions are reasonable (F1 typically has 20-22 cars)
        max_reasonable_position = 30  # Allow some flexibility
        too_high = (positions > max_reasonable_position).sum()
        
        total_positions = len(positions)
        high_pos_percentage = too_high / total_positions if total_positions > 0 else 0
        
        assert high_pos_percentage < 0.05, f"Too many unreasonably high positions: {high_pos_percentage:.1%} above position {max_reasonable_position}"
    
    def test_unique_race_driver_combinations(self):
        """Test: Each race-driver combination appears only once"""
        if self.labels_df.empty:
            pytest.skip("No labels data to test")
        
        # Check for duplicates
        duplicates = self.labels_df.duplicated(subset=['race_id', 'driver_id']).sum()
        
        assert duplicates == 0, f"Found {duplicates} duplicate race-driver combinations"
    
    def test_consistent_winners_and_positions(self):
        """Test: Race winners have position 1"""
        if (self.labels_df.empty or 
            'is_race_winner' not in self.labels_df.columns or 
            'race_position' not in self.labels_df.columns):
            pytest.skip("Missing data for winner-position consistency test")
        
        # Get records where both winner flag and position are available
        complete_data = self.labels_df[
            self.labels_df['is_race_winner'].notna() & 
            self.labels_df['race_position'].notna()
        ].copy()
        
        if len(complete_data) == 0:
            pytest.skip("No complete winner-position data to test")
        
        # Check: all race winners should have position 1
        winners = complete_data[complete_data['is_race_winner'] == 1]
        if len(winners) > 0:
            non_first_winners = (winners['race_position'] != 1).sum()
            assert non_first_winners == 0, f"Found {non_first_winners} race winners not in position 1"
        
        # Check: all position 1 finishers should be marked as winners
        first_place = complete_data[complete_data['race_position'] == 1]
        if len(first_place) > 0:
            non_winner_first = (first_place['is_race_winner'] != 1).sum()
            # Allow some flexibility for data quality issues
            assert non_winner_first < len(first_place) * 0.1, f"Found {non_winner_first} position 1 finishers not marked as winners"

def run_label_validation_report():
    """Generate a comprehensive label validation report"""
    print("=" * 60)
    print("LABEL VALIDATION REPORT")
    print("=" * 60)
    
    labels_file = Path('data/processed/labels.parquet')
    
    if not labels_file.exists():
        print("❌ Labels file not found. Run create_labels.py first.")
        return False
    
    labels_df = pd.read_parquet(labels_file)
    
    print(f"Labels Dataset Overview:")
    print(f"  Records: {len(labels_df):,}")
    print(f"  Columns: {len(labels_df.columns)}")
    print(f"  Unique races: {labels_df['race_id'].nunique():,}")
    print(f"  Unique drivers: {labels_df['driver_id'].nunique():,}")
    
    # Label distribution
    print(f"\nLabel Distribution:")
    
    if 'is_pole' in labels_df.columns:
        pole_count = labels_df['is_pole'].sum()
        pole_rate = pole_count / len(labels_df) * 100
        print(f"  Pole positions: {pole_count:,} ({pole_rate:.1f}%)")
    
    if 'is_race_winner' in labels_df.columns:
        win_count = labels_df['is_race_winner'].sum()
        win_rate = win_count / len(labels_df) * 100
        print(f"  Race wins: {win_count:,} ({win_rate:.1f}%)")
    
    # Top performers
    if 'is_pole' in labels_df.columns:
        print(f"\nTop Pole Position Holders:")
        pole_leaders = labels_df[labels_df['is_pole'] == 1]['driver_id'].value_counts().head()
        for i, (driver, count) in enumerate(pole_leaders.items(), 1):
            print(f"  {i}. {driver}: {count}")
    
    if 'is_race_winner' in labels_df.columns:
        print(f"\nTop Race Winners:")
        win_leaders = labels_df[labels_df['is_race_winner'] == 1]['driver_id'].value_counts().head()
        for i, (driver, count) in enumerate(win_leaders.items(), 1):
            print(f"  {i}. {driver}: {count}")
    
    # Data quality checks
    print(f"\nData Quality:")
    
    # Missing data
    missing_data = labels_df.isnull().sum()
    columns_with_missing = missing_data[missing_data > 0]
    
    if len(columns_with_missing) == 0:
        print(f"  ✅ No missing data")
    else:
        print(f"  ⚠️  Missing data in {len(columns_with_missing)} columns:")
        for col, count in columns_with_missing.items():
            percentage = count / len(labels_df) * 100
            print(f"    {col}: {count:,} ({percentage:.1f}%)")
    
    # Consistency checks
    if 'is_pole' in labels_df.columns:
        poles_per_race = labels_df.groupby('race_id')['is_pole'].sum()
        races_one_pole = (poles_per_race == 1).sum()
        total_races = len(poles_per_race)
        one_pole_rate = races_one_pole / total_races * 100
        print(f"  Races with exactly one pole: {races_one_pole}/{total_races} ({one_pole_rate:.1f}%)")
    
    if 'is_race_winner' in labels_df.columns:
        winners_per_race = labels_df.groupby('race_id')['is_race_winner'].sum()
        races_one_winner = (winners_per_race == 1).sum()
        total_races = len(winners_per_race)
        one_winner_rate = races_one_winner / total_races * 100
        print(f"  Races with exactly one winner: {races_one_winner}/{total_races} ({one_winner_rate:.1f}%)")
    
    return True

if __name__ == "__main__":
    # Run validation report
    run_label_validation_report()
    
    # Run pytest if requested
    if len(sys.argv) > 1 and sys.argv[1] == '--test':
        pytest.main([__file__, '-v'])